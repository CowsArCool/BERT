{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from d2l import torch as d2l\n",
    "import spacy\n",
    "from tqdm import tqdm, trange\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "from alive_progress import alive_bar, config_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "running_dir = r'C:\\Code\\NLP\\BERT\\Notebooks'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['wikitext-2']\n",
    "data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_object(path):\n",
    "    with open(path, 'rb') as inp:\n",
    "        return pickle.load(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_wiki(data_dir):\n",
    "    save_location = os.path.join(running_dir, 'paragraphs_data.pkl')\n",
    "    if os.path.isfile(save_location):\n",
    "        return load_object(save_location)\n",
    "\n",
    "    tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "    filepath = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    print(filepath)\n",
    "    with open(filepath, 'r', errors='replace') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    print('[INFO] File read')\n",
    "    \n",
    "    lines = lines[:1000]\n",
    "    print (len(lines))\n",
    "\n",
    "\n",
    "    paragraphs = [\n",
    "        [[token.text.lower() for token in tokenizer(sentence.strip())]\n",
    "         for sentence in line.strip().split('. ')]\n",
    "        for line in tqdm(lines)\n",
    "        if len(line.split('. ')) >= 2\n",
    "    ]\n",
    "\n",
    "    save_object(paragraphs, save_location)\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def clip_dicts(ds, max_len):\n",
    "    return [{k: d[k] for i, k in enumerate(d) if i <= max_len} for d in ds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "def _get_next_sentence(next_sentence, paragraphs):\n",
    "    \"\"\"Get next sentence from paragraphs . This function \n",
    "       randomly decided whether to use a correct or random next sentence\n",
    "       in the next sentence prediciton task. BERT will then predict whether\n",
    "       this sentence follows the previous or is new.\n",
    "\n",
    "    Args:\n",
    "        next_sentence (list): the origional next sentence\n",
    "        paragraphs (list[list[str]]): a list of sentences where sentences\n",
    "                                      is a list of tokens\n",
    "\n",
    "    Returns:\n",
    "        list: the next sentence in the sequence to predict on\n",
    "    \"\"\"\n",
    "    if np.random.rand() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        paragraph_idx = np.random.randint(0, high=len(paragraphs))\n",
    "        sentence_idx = np.random.randint(\n",
    "            0, high=len(paragraphs[paragraph_idx]))\n",
    "        next_sentence = paragraphs[paragraph_idx][sentence_idx]\n",
    "        is_next = False\n",
    "\n",
    "    return next_sentence, is_next\n",
    "\n",
    "\n",
    "def _get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"Get tokens of the BERT input sequence and their segment IDs .\n",
    "       This function is taken from the dl2 library but rewritten here\n",
    "       for clarity and to reduce reliance on dependecys .\n",
    "       \"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments\n",
    "\n",
    "\n",
    "def _generate_nsp_data(paragraph, paragraphs, max_len):\n",
    "    \"\"\"Generate a list of tokens for a given paragraph .\n",
    "\n",
    "    Args:\n",
    "        paragraph (list): list of words in paragraph\n",
    "        paragraphs (list): list of paragraphs (obviously)\n",
    "        max_len (int): maximum length for a single example\n",
    "\n",
    "    Returns:\n",
    "        list: list of token, segment tuples \n",
    "    \"\"\"\n",
    "    nsp_data = list()\n",
    "    # len(paragraph)-1 because it grabs next sentence wtih [i+1] indexing\n",
    "    for i in range(len(paragraph)-1):\n",
    "        # chooses whether to use the true next sentence or a random one\n",
    "        # BERT will try to distinguish between the two in its nsp task\n",
    "        sentence = paragraph[i]\n",
    "        next_sentence, is_next = _get_next_sentence(paragraph[i+1], paragraphs)\n",
    "\n",
    "        if len(sentence) + len(next_sentence) > max_len:\n",
    "            continue\n",
    "\n",
    "        tokens, segments = _get_tokens_and_segments(sentence, next_sentence)\n",
    "\n",
    "        nsp_data.append((tokens, segments, is_next))\n",
    "\n",
    "    return nsp_data\n",
    "\n",
    "\n",
    "def _replace_masked_tokens(tokens, possible_prediction_indexes, num_preds, vocab):\n",
    "    \"\"\"Replace tokens to be masked with either \"<mask>\", the correct word, \n",
    "       or a random word selected from vocab.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): unmasked tokens to be replaced partially with masked ones\n",
    "        possible_prediction_indexes ([type]): indexes of non-token characters\n",
    "                                              to possibly replace with <mask>\n",
    "                                              or random word tokens\n",
    "        num_preds (int): number of tokens to theoretically replace\n",
    "                            with <mask>\n",
    "        vocab (Vocab): vocabulary object for the dataset\n",
    "\n",
    "    Returns:\n",
    "        mlm_tokens (list): tokens input but with some tokens\n",
    "                           replaced wtih masks and random tokens\n",
    "        pred_lables_and_positions (list[tuples]): list containing pairs of \n",
    "                                                  indexes and their tokens \n",
    "    \"\"\"\n",
    "    mlm_tokens = [token for token in tokens]\n",
    "    pred_labels_and_positions = list()\n",
    "\n",
    "    for i, idx in enumerate(possible_prediction_indexes):\n",
    "        if i >= num_preds:\n",
    "            break\n",
    "\n",
    "        random_num = np.random.random()*100\n",
    "        if random_num < 80:\n",
    "            masked_token = '<mask>'\n",
    "\n",
    "        elif random_num < 90:\n",
    "            masked_token = tokens[idx]\n",
    "\n",
    "        else:\n",
    "            masked_token = np.random.choice(list(vocab.stoi.keys()))\n",
    "\n",
    "        mlm_tokens[idx] = masked_token\n",
    "        # Pred labels and positions stores unamsked ground truth y values\n",
    "        # thats why it indexes tokens instead of mlm_tokens which has been modified\n",
    "        # with masks\n",
    "        pred_labels_and_positions.append((idx, tokens[idx]))\n",
    "\n",
    "    return mlm_tokens, pred_labels_and_positions\n",
    "\n",
    "\n",
    "def _generate_mlm_data_from_tokens(tokens, vocab):\n",
    "    # tokens is a list of strings\n",
    "    possible_prediction_indexes = list()\n",
    "    # Finds all indexes not containg cls and sep tokens\n",
    "    # making them potential masked tokens\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in ['<cls>', '<sep>']:\n",
    "            possible_prediction_indexes.append(i)\n",
    "\n",
    "    # Mask 15% of tokens or at minimum 1\n",
    "    num_masked_tokens = max(1, len(possible_prediction_indexes)*0.15)\n",
    "    masked_mlm_tokens, pred_labels_and_positions = _replace_masked_tokens(\n",
    "        tokens, possible_prediction_indexes, num_masked_tokens, vocab\n",
    "    )  # takes unmasked tokens and masks \"num_masked_tokens of them\" at positions \"possible_pred_positions\"\n",
    "\n",
    "    pred_labels_and_positions = sorted(\n",
    "        pred_labels_and_positions,\n",
    "        key=lambda x: x[0]\n",
    "    )  # sort by indexes replaced tokens\n",
    "\n",
    "    pred_positions = [idx[0] for idx in pred_labels_and_positions]\n",
    "    pred_ground_truth = [idx[1] for idx in pred_labels_and_positions]\n",
    "\n",
    "    # calling vocab () int incodes token paramater\n",
    "    return (vocab(masked_mlm_tokens), pred_positions, vocab(pred_ground_truth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvalidVocabLengths (Exception):\n",
    "    behavior = 'raise'\n",
    "\n",
    "    def __init__(self, stoi, itos):\n",
    "        behavior = self.__class__.behavior\n",
    "        if behavior == 'ignore':\n",
    "            return\n",
    "\n",
    "        elif behavior == 'warn':\n",
    "            warnings.warn(\n",
    "                f'Invalid Vocab stoi/itos lengths; len(stoi): {len(stoi)}, len(itos): {len(itos)}')\n",
    "\n",
    "        elif behavior == 'raise':\n",
    "            raise\n",
    "\n",
    "\n",
    "class DatasetWrapper:\n",
    "    \"\"\"A wrapper for Dataset classes and the DatasetAssember class .\n",
    "       The idea is to make a generalizable class to help visualize\n",
    "       and process datasets just by inherritence .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return '{} (Datasets: (\\n\\t{}))'.format(self.__class__.__name__, '\\n\\t'.join(\n",
    "            [f'{key}: {value}'for key, value in self.datasets.items()]))\n",
    "\n",
    "    def visualize_vocab(self, n=30):\n",
    "        # Entirely untested but theoretically working .\n",
    "        # Can only be tried once dataset initilization is completed\n",
    "        # TODO: complete data visualization with avg word lengths and\n",
    "        #       sentence length plots .\n",
    "        active_vocab = self.vocab\n",
    "        frequencies = self.frequencies\n",
    "\n",
    "        frequencies_df = pd.DataFrame(frequencies).reset_index(drop=False)\n",
    "        print(frequencies_df)\n",
    "        frequencies_df.plot.bar()\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "class DatasetAssembler (DatasetWrapper):\n",
    "    \"\"\"Allows support for multiple datasets with advanced\n",
    "       processing and splits . Could theoretically allow for\n",
    "       cross validation however that is currently not implemented .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, paragraphs, **kwargs) -> None:\n",
    "        \"\"\"Initialize mutliple datasets to a single object \n",
    "           with stored vocabulary .\n",
    "\n",
    "        Args:\n",
    "            paragraphs (array): List holding paragraph data\n",
    "            **kwargs:\n",
    "                splits (list[percents]): list of percents to split\n",
    "                                         data into . \n",
    "\n",
    "                reserved_tokens (list[str]): list of tokens to \n",
    "                                             reserve for nlp modeling\n",
    "        \"\"\"\n",
    "\n",
    "        # Theoretical paragraph preprocessing\n",
    "\n",
    "        sentences = [\n",
    "            sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "\n",
    "        self.vocab = Vocab(sentences, **kwargs)\n",
    "\n",
    "        if 'splits' not in kwargs.keys():\n",
    "            kwargs['splits'] = [1]\n",
    "        # splits should be a list of percentages to split data into\n",
    "        assert np.sum(kwargs['splits']) == 1, \\\n",
    "            'Invalid split percentages; splits must add to 100%'\n",
    "\n",
    "        self.splits = list()\n",
    "        for i, split_percent in enumerate(kwargs['splits']):\n",
    "            prev_split = np.sum(kwargs['splits'][:i])\n",
    "\n",
    "            print(f'prev split {prev_split}, split percent: {split_percent}')\n",
    "            print(\n",
    "                f'term 1: {int(prev_split*len(paragraphs))}, term 2: {int(prev_split*len(paragraphs)+split_percent*len(paragraphs))}')\n",
    "            self.splits.append(\n",
    "                WikiTextDataset(\n",
    "                    paragraphs[int(prev_split*len(paragraphs)):int(prev_split *\n",
    "                                                                   len(paragraphs)+split_percent*len(paragraphs))],\n",
    "                    self.vocab,\n",
    "                    split_percent,\n",
    "                    max_example_len=kwargs['max_example_len'] if 'max_example_len' in list(\n",
    "                        kwargs.keys()) else 60\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if len(self.splits) <= 3:\n",
    "            # complicated syntax making it possible to assign all three at once while padding\n",
    "            # validset/testset if there arent enough splits to fill those values\n",
    "            self._trainset, self._validset, self._testset = [\n",
    "                split for split in self.splits] + [None]*(3 - len(self.splits))\n",
    "\n",
    "        self.datasets = {\n",
    "            'train': self._trainset,\n",
    "            'valid': self._validset,\n",
    "            'test': self._testset\n",
    "        }\n",
    "\n",
    "    def build_loaders(self, batch_size, shuffle, num_workers):\n",
    "        self.dataloaders = {}\n",
    "\n",
    "        for name, dataset in self.datasets.items():\n",
    "            if dataset is not None:\n",
    "                loader = torch.utils.data.DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    num_workers=num_workers\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                loader = None\n",
    "\n",
    "            self.dataloaders[name] = loader\n",
    "\n",
    "        return self.dataloaders\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, str):\n",
    "            return self.datasets[idx]\n",
    "\n",
    "        elif isinstance(idx, int):\n",
    "            return self.splits[idx]\n",
    "\n",
    "        else:\n",
    "            raise IndexError(f'Invalid dataset idx type {type(idx)}')\n",
    "\n",
    "    # More methods to access datasets\n",
    "    @property\n",
    "    def trainset(self): return self._trainset\n",
    "\n",
    "    @property\n",
    "    def validset(self): return self._validset\n",
    "\n",
    "    @property\n",
    "    def testset(self): return self._testset\n",
    "\n",
    "    @property\n",
    "    def trainloader(self):\n",
    "        return self.dataloaders['train']\n",
    "\n",
    "    @property\n",
    "    def testloader(self):\n",
    "        return self.dataloaders['test']\n",
    "\n",
    "    @property\n",
    "    def validloader(self):\n",
    "        return self.dataloaders['valid']\n",
    "\n",
    "\n",
    "class WikiTextDataset (Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        paragraphs,\n",
    "        vocab,\n",
    "        dataset_percent,\n",
    "        max_example_len=None \n",
    "    ):\n",
    "        # dataset_percent is purely for accesibility\n",
    "        # allows for more readable printing with known\n",
    "        # percentage of full dataset\n",
    "        self.dataset_percent = dataset_percent\n",
    "        self.vocab = vocab\n",
    "        self.paragraphs = paragraphs\n",
    "        self.max_example_len = max_example_len\n",
    "        print (f'max example len: {self.max_example_len}')\n",
    "\n",
    "        training_examples = list()\n",
    "        for paragraph in paragraphs:\n",
    "            training_examples.extend(\n",
    "                _generate_nsp_data(\n",
    "                    paragraph,\n",
    "                    paragraphs,\n",
    "                    max_example_len\n",
    "                )\n",
    "            )\n",
    "\n",
    "        training_examples = [(_generate_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next))\n",
    "                             for tokens, segments, is_next in tqdm(training_examples, desc='Building Training Examples')]\n",
    "\n",
    "        # A tiny bit of cheating with the d2l library\n",
    "        # Made padding significantly easier\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights, self.all_mlm_labels,\n",
    "         self.nsp_labels) = d2l._pad_bert_inputs(training_examples, max_example_len, self.vocab)\n",
    "\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = [self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx]]\n",
    "\n",
    "        for i, tensor in enumerate(data): \n",
    "            if len(tensor.shape) > 0:\n",
    "                if tensor.shape[0] > self.max_example_len:\n",
    "                    data[i] = tensor[:self.max_example_len]\n",
    "                    # print ('hi')\n",
    "\n",
    "        return tuple(data)\n",
    "                \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'WikiTextDataset ({self.dataset_percent*100}% of full dataset)'\n",
    "\n",
    "\n",
    "class Vocab ():\n",
    "    \"\"\"Universal text vocabulary class .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentences,\n",
    "        reserved_tokens=None,\n",
    "        max_vocab_size=np.inf,\n",
    "        frequency_threshold=2,\n",
    "        splits=None, # a little bit of poor coding practices\n",
    "        max_example_len=None # using this to allow for kwargs in Assembler\n",
    "        # **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize vocabulary from a list of sentences .\n",
    "\n",
    "        Args:\n",
    "            sentences (list): sentences derived from paragraphs array\n",
    "            reserved_tokens (list, optional): Tokens to not build in initializtion. Defaults to None.\n",
    "            max_vocab_size (int, optional): maximum number of vocab tokens to build. Defaults to infinite tokens.\n",
    "            frequency_threshold (int, optional): min number of time a word must appear to be placed in vocab. Defaults to 2.\n",
    "\n",
    "        Raises:\n",
    "            InvalidVocabLengths: If vocab lengths are somehow altered so that itos and stoi\n",
    "                                 are not exactly opposite eachother\n",
    "        \"\"\"\n",
    "        text = []\n",
    "        for sentence in sentences:\n",
    "            text.extend([word for word in sentence])\n",
    "\n",
    "        frequencies = {}\n",
    "        self.stoi = {token: i for i, token in enumerate(reserved_tokens)}\n",
    "        self.itos = {v: k for k, v in self.stoi.items()}\n",
    "\n",
    "        for word in text:\n",
    "            if word in frequencies:\n",
    "                frequencies[word] += 1\n",
    "            else:\n",
    "                frequencies[word] = 1\n",
    "\n",
    "        self.sorted_frequencies = dict(\n",
    "            sorted(frequencies.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        idx = len(self.stoi)\n",
    "        for word, freq in self.sorted_frequencies.items():\n",
    "            if word not in self.stoi and freq >= frequency_threshold:\n",
    "\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "\n",
    "                idx += 1\n",
    "\n",
    "        if len(self.stoi) != len(self.itos):\n",
    "            raise InvalidVocabLengths(self.stoi, self.itos)\n",
    "\n",
    "        if len(self.stoi) > max_vocab_size:\n",
    "            self.stoi, self.itos = clip_dicts(\n",
    "                (self.stoi, self.itos), max_vocab_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # allows a token to be fetched\n",
    "        # either by idx or word string\n",
    "\n",
    "        if isinstance(idx, str):\n",
    "            return self.stoi[idx]\n",
    "\n",
    "        elif isinstance(idx, int):\n",
    "            return self.itos[idx]\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f'Invalid index {idx}')\n",
    "\n",
    "    def __call__(self, tokens):\n",
    "        # int encodes list of tokens\n",
    "        assert all([isinstance(token, str) for token in tokens]), \\\n",
    "            'tokens list does not contain strings'\n",
    "\n",
    "        int_encoded_tokens = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            try:\n",
    "                int_encoded_tokens.append(self.stoi[token])\n",
    "\n",
    "            except KeyError:\n",
    "                print(f'KeyError at {i} token: {token}')\n",
    "\n",
    "        # int_encoded_tokens = [self.stoi[token] for token in tokens]\n",
    "        return int_encoded_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.stoi) != len(self.itos):\n",
    "            raise InvalidVocabLengths(self.stoi, self.itos)\n",
    "        return len(self.stoi.keys())\n",
    "\n",
    "    def __str__(self):\n",
    "        return ('Vocab: (\\nlength: {}\\n{})'\n",
    "                .format(self.__len__(), list(self.stoi.keys())))\n",
    "\n",
    "    def stoi(self, string):\n",
    "        return self.stoi[string]\n",
    "\n",
    "    def itos(self, idx):\n",
    "        return self.itos[idx]\n",
    "\n",
    "\n",
    "def load_wikitext():\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    assembler = DatasetAssembler(\n",
    "        paragraphs,\n",
    "        splits=[0.8, 0.2],\n",
    "        reserved_tokens=['<sep>', 'unk', '<cls>', '<mask>', '<pad>'],\n",
    "        frequency_threshold=0,\n",
    "        max_example_len=120\n",
    "    )\n",
    "\n",
    "    return assembler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assembler loaded from save\n"
     ]
    }
   ],
   "source": [
    "# optional assembler name for choosing assembler saves\n",
    "# appended to \"assembler {}\" when finding file name\n",
    "ASSEMBLER_NAME = '64' \n",
    "\n",
    "assembler_save_path = os.path.join(running_dir, f'assembler{ASSEMBLER_NAME}.pkl')\n",
    "\n",
    "if os.path.isfile(assembler_save_path):\n",
    "    assembler = load_object(assembler_save_path)\n",
    "    print('assembler loaded from save')\n",
    "\n",
    "\n",
    "else:\n",
    "    assembler = load_wikitext()\n",
    "    batch_size = 32\n",
    "    shuffle = False\n",
    "    num_workers = 0\n",
    "\n",
    "    assembler.build_loaders(batch_size, shuffle, num_workers)\n",
    "\n",
    "    save_object(assembler, assembler_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraphs = (_read_wiki(data_dir))\n",
    "# paragraphs = paragraphs[:200]\n",
    "# assembler = DatasetAssembler (paragraphs, reserved_tokens = ['<sep>','unk','<cls>','<mask>','<pad>'], splits = [0.9, 0.1], frequency_threshold=0)\n",
    "# assembler.build_loaders(512, False, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = assembler.trainloader\n",
    "validloader = assembler.validloader\n",
    "testloader = assembler.testloader\n",
    "print(len(trainloader))\n",
    "\n",
    "for i, batch in enumerate (trainloader):\n",
    "    for data in batch:\n",
    "        print (data.shape)\n",
    "\n",
    "    print ('')\n",
    "\n",
    "    if i > 0:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (\n",
    "        tokens_X,\n",
    "        segments_X,\n",
    "        valid_lens_x,\n",
    "        pred_positions_X,\n",
    "        mlm_weights_X, mlm_Y,\n",
    "        nsp_y)\\\n",
    "        in trainloader:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    print (segments_X)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = d2l.MultiHeadAttention(key_size, query_size,\n",
    "                                                value_size, num_hiddens,\n",
    "                                                num_heads, dropout, use_bias)\n",
    "\n",
    "        # Steal d2l multi head attention # TODO: write mutli headed attention myself\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(norm_shape)\n",
    "        self.norm2 = nn.LayerNorm(norm_shape)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.ff = SimpleFF(\n",
    "            ffn_num_input,\n",
    "            ffn_num_hiddens,\n",
    "            num_hiddens\n",
    "        )\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.norm1(\n",
    "            self.dropout(\n",
    "                self.attention(X, X, X, valid_lens)\n",
    "            ) + X\n",
    "        )\n",
    "\n",
    "        ff_out = self.ff(Y)\n",
    "\n",
    "        return self.norm2(self.dropout(ff_out) + Y)\n",
    "\n",
    "\n",
    "class SimpleFF (nn.Module):\n",
    "    def __init__(self, ff_num_input, ff_num_hiddens, ff_out):\n",
    "\n",
    "        super(SimpleFF, self).__init__()\n",
    "\n",
    "        # Simple feed forward network\n",
    "        self.feed_forward = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(ff_num_input, ff_num_hiddens),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(ff_num_hiddens, ff_out)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, Y):\n",
    "        for layer in self.feed_forward:\n",
    "            Y = layer(Y)\n",
    "\n",
    "        return Y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, num_hiddens, norm_shape,\n",
    "        ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "        num_layers, dropout, max_len=1000, key_size=768,\n",
    "        query_size=768, value_size=768, **kwargs\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blocks.add_module(\n",
    "                \"{}\".format(i),\n",
    "\n",
    "                EncoderBlock(\n",
    "                    key_size,\n",
    "                    query_size,\n",
    "                    value_size,\n",
    "                    num_hiddens,\n",
    "                    norm_shape,\n",
    "                    ffn_num_input,\n",
    "                    ffn_num_hiddens,\n",
    "                    num_heads,\n",
    "                    dropout,\n",
    "                    True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # positional embedding is learnable\n",
    "        # so we make it a param\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X\n",
    "\n",
    "\n",
    "class NSP (nn.Module):\n",
    "    \"\"\" About the simplest possible network\n",
    "    This class handles the Next Sentence Prediction\n",
    "    output layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(NSP, self).__init__()\n",
    "\n",
    "        self.ff = nn.Linear(input_size, 2)\n",
    "        # Outputs percentages\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class MLM (nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hiddens,\n",
    "        num_inputs=768,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MLM, self).__init__()\n",
    "\n",
    "        # Will be applied sequentially\n",
    "        self.ff1 = nn.Linear(num_inputs, hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(hiddens)\n",
    "        self.ff2 = nn.Linear(hiddens, vocab_size)\n",
    "\n",
    "    def forward(self, x, preds):\n",
    "        # dim 0 is batch size so dim 1 is the actual values\n",
    "        num_preds = preds.size(1)\n",
    "        # Flattens pred positions\n",
    "        positions = preds.reshape(-1)\n",
    "        N = x.size(0)  # batch size\n",
    "\n",
    "        # batch idx is a bit confusing but the idea is that you would have\n",
    "        # an array of (0... batch_size) each number in the sequence is repeated\n",
    "        # num_predds times using repeat_interleave\n",
    "        batch_idx = torch.arange(0, N)\n",
    "        batch_idx = batch_idx.repeat_interleave(num_preds)\n",
    "\n",
    "        x_mask = (x[batch_idx, positions]\n",
    "                  .reshape((N, num_preds, -1))\n",
    "                  )\n",
    "\n",
    "        out = (\n",
    "            self.ff2(\n",
    "                self.norm(\n",
    "                    self.relu(\n",
    "                        self.ff1(x_mask)\n",
    "                    ))))  # applied layers sequentially\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BERT (nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, num_hiddens,\n",
    "        norm_shape, ffn_num_input,\n",
    "        ffn_num_hiddens, num_heads,\n",
    "        num_layers, dropout,\n",
    "        max_len=1000, key_size=768,\n",
    "        query_size=768, value_size=768,\n",
    "        hidden_in_features=768,\n",
    "        mlm_in_features=768,\n",
    "        nsp_in_features=768\n",
    "    ):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size, num_hiddens, norm_shape,\n",
    "            ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "            num_layers, dropout,\n",
    "            max_len=max_len,\n",
    "            key_size=key_size,\n",
    "            query_size=query_size,\n",
    "            value_size=value_size\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(hidden_in_features, num_hiddens),\n",
    "            nn.Tanh()  # hyperbolic tangent activation function\n",
    "        )\n",
    "\n",
    "        # Output layers for mlm and nsp\n",
    "        self.nsp = NSP(nsp_in_features)\n",
    "        self.mlm = MLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens,\n",
    "        segments,\n",
    "        valid_lens=None,\n",
    "        pred_positions=None\n",
    "    ):\n",
    "        X = self.encoder(tokens, segments, valid_lens)\n",
    "\n",
    "        if not pred_positions is None:\n",
    "            mlm_pred = self.mlm(X, pred_positions)\n",
    "\n",
    "        else:\n",
    "            mlm_pred = None\n",
    "\n",
    "        nsp_pred = self.nsp(\n",
    "            self.hidden(\n",
    "                X[:, 0, :]\n",
    "                # 0 index is cls token\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return X, mlm_pred, nsp_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert_med'\n",
    "\n",
    "bert_models_params = {\n",
    "    'tiny_bert': {\n",
    "        'vocab_size':len(assembler.vocab),\n",
    "        'num_hiddens':128,\n",
    "        'norm_shape':[128],\n",
    "        'ffn_num_input':128,\n",
    "        'ffn_num_hiddens':256,\n",
    "        'num_heads':2,   \n",
    "        'num_layers':2,\n",
    "        'dropout':0.2,\n",
    "        'key_size':128,\n",
    "        'query_size':128,\n",
    "        'value_size':128,\n",
    "        'hidden_in_features':128,\n",
    "        'mlm_in_features':128,\n",
    "        'nsp_in_features':128\n",
    "    },\n",
    "\n",
    "    'bert_med': {\n",
    "        'vocab_size':len(assembler.vocab),\n",
    "        'num_hiddens':128,\n",
    "        'norm_shape':[128],\n",
    "        'ffn_num_input':128,\n",
    "        'ffn_num_hiddens':256,\n",
    "        'num_heads':8,   \n",
    "        'num_layers':4,\n",
    "        'dropout':0.2,\n",
    "        'key_size':128,\n",
    "        'query_size':128,\n",
    "        'value_size':128,\n",
    "        'hidden_in_features':128,\n",
    "        'mlm_in_features':128,\n",
    "        'nsp_in_features':128\n",
    "    }\n",
    "}\n",
    "\n",
    "# Allows model choice from bert_models_param dict\n",
    "# Input chosen model into MODEL_NAME constant\n",
    "model = BERT(**bert_models_params[MODEL_NAME])\n",
    "\n",
    "\n",
    "for (\n",
    "        tokens_X,\n",
    "        segments_X,\n",
    "        valid_lens_x,\n",
    "        pred_positions_X,\n",
    "        mlm_weights_X, mlm_Y,\n",
    "        nsp_y)\\\n",
    "        in trainloader:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "\n",
    "    out = model(tokens_X, segments_X,\n",
    "               valid_lens_x.reshape(-1), pred_positions_X)\n",
    "    for obj in out:\n",
    "        print(obj.shape)\n",
    "\n",
    "    break\n",
    "# POGGERS IT WOKRS\n",
    "# YAYAYAYAYYAYAYYYYYYYYYYYYYYYYYYY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0008) # origionally 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "max_norm = None\n",
    "\n",
    "print (f'BERT Model loaded with device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB BLOCK\n",
    "wandb_params = bert_models_params[MODEL_NAME].copy()\n",
    "wandb_params['Model Name']= MODEL_NAME\n",
    "wandb.init(project=\"BERT\", entity=\"cowsarecool\", config=wandb_params)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures a save location for runnning models\n",
    "save_dir = os.path.join (running_dir, 'model_saves', MODEL_NAME)\n",
    "if not os.path.isdir (save_dir):\n",
    "    os.makedirs (save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from save\n",
    "MODEL_SAVE_EPOCH = -1\n",
    "\n",
    "\n",
    "save_dict_path = os.path.join(save_dir, 'epoch_{}'.format(MODEL_SAVE_EPOCH))\n",
    "if os.path.isfile (save_dict_path):\n",
    "    save_state_dicts = torch.load(save_dict_path)\n",
    "    model.load_state_dict(save_state_dicts['model_state'])\n",
    "\n",
    "else:\n",
    "    print ('file does not exist, or is unspecified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN TESTING\n",
    "test_iterations = 100 # <<<<--------- TIM\n",
    "\n",
    "print_every = 5\n",
    "save_every = 30 \n",
    "eval_every = 10\n",
    "\n",
    "\n",
    "runtimes = list()\n",
    "    # with alive_bar(test_iterations+1, ## Only works in a normal python script\n",
    "    #                        title='Training', bar='smooth',\n",
    "    #                        length=75) as bar:\n",
    "try:\n",
    "\n",
    "    for epoch in tqdm_notebook(range(test_iterations+1), desc='Training'):\n",
    "        cumulative_losses, nsp_losses, mlm_losses = [], [], []\n",
    "        nsp_accuracies, mlm_accuracies = [], []\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        model.train()\n",
    "        for batch_num, (\n",
    "            tokens_X,\n",
    "            segments_X,\n",
    "            valid_lens_x,\n",
    "            pred_positions_X,\n",
    "            mlm_weights_X, mlm_Y,\n",
    "            nsp_y)\\\n",
    "        in enumerate(trainloader):\n",
    "            # print ('\\n'.join ([str(data.shape) for data in [tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y]]))\n",
    "            \n",
    "            # Sending absolutely everything to gpu \n",
    "            # Unfortunately there is no way to do this with a loop\n",
    "            # so we're stuck manually sending each variable to device\n",
    "            tokens_X, segments_X,\\\n",
    "            valid_lens_x, pred_positions_X,\\\n",
    "            mlm_weights_X, mlm_Y, nsp_y =(\n",
    "            tokens_X.to(device), segments_X.to(device),\n",
    "            valid_lens_x.to(device), pred_positions_X.to(device),\n",
    "            mlm_weights_X.to(device), mlm_Y.to(device), nsp_y.to(device))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X, mlm_y_hat, nsp_y_hat =  model(tokens_X, segments_X,\n",
    "                valid_lens_x.reshape(-1), pred_positions_X)\n",
    "            \n",
    "            # nsp loss calculation\n",
    "            nsp_loss = criterion (nsp_y_hat, nsp_y)\n",
    "\n",
    "            # mlm loss is a bit more complicated; it involves\n",
    "            # mlm weights, a one hot encoded vector specifying which \n",
    "            # tokens in the mlm input are <pad>. This is used to factor\n",
    "            # padding out of the loss calculation. Another complication \n",
    "            # is the multidimensionality of the mlm output, it just needs\n",
    "            # flattening though before going through the loss funciton (criterion)\n",
    "            mlm_loss = criterion (\n",
    "                mlm_y_hat.reshape(-1, len(trainloader.dataset.vocab)),\n",
    "                mlm_Y.reshape(-1)\n",
    "            ) * mlm_weights_X.reshape(-1, 1) # factors out padding\n",
    "            mlm_loss = mlm_loss.sum()/(mlm_weights_X.sum()+1e-8)\n",
    "\n",
    "            cumulative_loss = mlm_loss + nsp_loss\n",
    "\n",
    "            cumulative_losses.append (cumulative_loss.item()) \n",
    "            mlm_losses.append (mlm_loss.item())\n",
    "            nsp_losses.append (nsp_loss.item())\n",
    "\n",
    "            nsp_accuracy = torch.sum(\n",
    "                torch.argmax(nsp_y_hat, dim=1) == nsp_y\n",
    "            )/len(nsp_y_hat)\n",
    "\n",
    "            mlm_accuracy = torch.sum(\n",
    "                torch.argmax(mlm_y_hat, dim=2) == mlm_Y\n",
    "            ) / np.product(list(mlm_Y.shape))       \n",
    "\n",
    "            nsp_accuracies.append (nsp_accuracy.item())\n",
    "            mlm_accuracies.append (mlm_accuracy.item())\n",
    "\n",
    "            # print (f'nsp y_hat: {nsp_y_hat.shape}, nsp_y: {nsp_y.shape}')\n",
    "            # print (f'mlm y_hat: {mlm_y_hat.shape}, mlm_y: {mlm_Y.shape}')\n",
    "            if max_norm is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "            # cumulative_loss.backward()\n",
    "            mlm_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            wandb.log ({\n",
    "                'mlm_loss':mlm_loss,\n",
    "                'nsp_loss':nsp_loss,\n",
    "                'cumulative_loss':cumulative_loss,\n",
    "                'nsp_accuracy':nsp_accuracy,\n",
    "                'mlm_accuracy':mlm_accuracy\n",
    "            })\n",
    "            \n",
    "            # Unfortunately Alivebar doesnt work in notebooks\n",
    "            # Freaking Sucks\n",
    "            # bar.text(\n",
    "            #     f'Batch Accuracy: {mlm_accuracy:.3f}\\t Batch Num: {batch_num}')\n",
    "            \n",
    "        runtimes.append(time.perf_counter()-start_time)\n",
    "        if (epoch % print_every) ==0:\n",
    "            (print (\n",
    "                'Epoch: {}\\t Cum Loss: {:.3f}, MLM Loss: {:.3f}, NSP Loss: {:.3f}\\t \\\n",
    "MLM Accuracy: {:.3f}, NSP Accuracy {:.3f}\\t \\\n",
    "Avg Epoch Runtime: {:.3f}'\n",
    "            .format(*[\n",
    "                np.mean(data) for data in [\n",
    "                    epoch, \n",
    "                    cumulative_losses, \n",
    "                    mlm_losses, \n",
    "                    nsp_losses, \n",
    "                    mlm_accuracies, \n",
    "                    nsp_accuracies, \n",
    "                    runtimes\n",
    "                    ]\n",
    "                ]\n",
    "            )))\n",
    "\n",
    "            runtimes = list()\n",
    "        \n",
    "        if (epoch % save_every)==0:\n",
    "            model_state = model.state_dict()\n",
    "            optimizer_state = optimizer.state_dict()\n",
    "\n",
    "            torch.save({\n",
    "                'model_state':model_state,\n",
    "                'optimizer_state':optimizer_state\n",
    "                }, os.path.join(save_dir, f'epoch_{epoch}'))\n",
    "\n",
    "        if (epoch % eval_every == 0):\n",
    "            if validloader is None:\n",
    "                # if validloader contains nothing\n",
    "                # dont evaluate\n",
    "                continue\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                valid_cumulative_losses, valid_nsp_accuracies, valid_mlm_accuracies = [], [], []\n",
    "\n",
    "                for (\n",
    "                    tokens_X,\n",
    "                    segments_X,\n",
    "                    valid_lens_x,\n",
    "                    pred_positions_X,\n",
    "                    mlm_weights_X, mlm_Y,\n",
    "                    nsp_y)\\\n",
    "                in validloader:\n",
    "                    tokens_X, segments_X,\\\n",
    "                    valid_lens_x, pred_positions_X,\\\n",
    "                    mlm_weights_X, mlm_Y, nsp_y =(\n",
    "                    tokens_X.to(device), segments_X.to(device),\n",
    "                    valid_lens_x.to(device), pred_positions_X.to(device),\n",
    "                    mlm_weights_X.to(device), mlm_Y.to(device), nsp_y.to(device))\n",
    "\n",
    "                    X, mlm_y_hat, nsp_y_hat =  model(tokens_X, segments_X,\n",
    "                        valid_lens_x.reshape(-1), pred_positions_X)\n",
    "\n",
    "                    nsp_loss = criterion (nsp_y_hat, nsp_y)\n",
    "                    mlm_loss = criterion (\n",
    "                        mlm_y_hat.reshape(-1, len(trainloader.dataset.vocab)),\n",
    "                        mlm_Y.reshape(-1)\n",
    "                    ) * mlm_weights_X.reshape(-1, 1) # factors out padding\n",
    "                    mlm_loss = mlm_loss.sum()/(mlm_weights_X.sum()+1e-8)\n",
    "\n",
    "                    cumulative_loss = mlm_loss + nsp_loss\n",
    "\n",
    "                    nsp_accuracy = torch.sum(\n",
    "                        torch.argmax(nsp_y_hat, dim=1) == nsp_y\n",
    "                    )/len(nsp_y_hat)\n",
    "\n",
    "                    mlm_accuracy = torch.sum(\n",
    "                        torch.argmax(mlm_y_hat, dim=2) == mlm_Y\n",
    "                    )/ np.product(list(mlm_Y.shape))           \n",
    "\n",
    "                    valid_cumulative_losses.append(cumulative_loss.item())\n",
    "                    valid_nsp_accuracies.append(nsp_accuracy.item())\n",
    "                    valid_mlm_accuracies.append(mlm_accuracy.item())\n",
    "\n",
    "                wandb.log ({\n",
    "                    'valid_cumulative_loss':np.mean(valid_cumulative_losses),\n",
    "                    'valid_nsp_accuracy':np.mean(valid_nsp_accuracies),\n",
    "                    'valid_mlm_accuracy':np.mean(valid_mlm_accuracies)\n",
    "                })\n",
    "\n",
    "                print (\n",
    "                    'Eval Epoch: {}\\t MLM Acc: {}, NSP Acc: {}, Cum Loss: {}'\n",
    "                    .format(*\n",
    "                        [\n",
    "                            np.round(np.mean(data), decimals=2)\n",
    "                            for data in [epoch, valid_mlm_accuracies, valid_nsp_accuracies, valid_cumulative_losses]\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "except KeyboardInterrupt:\n",
    "    print (\"Keyboard Interrupt Detected\\n Ending Run\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if (' '.join(str(e).split(' ')[:4])) != 'CUDA out of memory.':\n",
    "        raise e \n",
    "    \n",
    "    else:\n",
    "        print ('CUDA NEEDS MORE FREAKING MEMORY\\nProbably about {} more'.format(''.join(str(e).split(' ')[7:9])))\n",
    "\n",
    "else:\n",
    "    print (\"Program Finished Successfully\")\n",
    "\n",
    "finally:\n",
    "    wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_encode (text):\n",
    "    pass # TODO: write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstration (model, validloader):\n",
    "    model.eval()\n",
    "    for (tokens_X,\n",
    "         segments_X,\n",
    "         valid_lens_x,\n",
    "         pred_positions_X,\n",
    "         mlm_weights_X, mlm_Y,\n",
    "         nsp_y) in validloader:\n",
    "        \n",
    "        tokens_X, segments_X,\\\n",
    "        valid_lens_x, pred_positions_X,\\\n",
    "        mlm_weights_X, mlm_Y, nsp_y =(\n",
    "            tokens_X.to(device), segments_X.to(device),\n",
    "            valid_lens_x.to(device), pred_positions_X.to(device),\n",
    "            mlm_weights_X.to(device), mlm_Y.to(device), nsp_y.to(device)\n",
    "            )\n",
    "\n",
    "        _, mlm_y_hat, nsp_y_hat =  model(tokens_X, segments_X,\n",
    "            valid_lens_x.reshape(-1), pred_positions_X)\n",
    "\n",
    "        vector_itos = np.vectorize (lambda x: validloader.dataset.vocab.itos[x])\n",
    "        cpu_device = torch.device('cpu')\n",
    "        segments_X, tokens_X, mlm_y_hat, mlm_Y = (segments_X.to(cpu_device), \n",
    "            tokens_X.to(cpu_device), mlm_y_hat.to(cpu_device), \n",
    "            mlm_Y.to(cpu_device))\n",
    "\n",
    "        nsp_data = pd.DataFrame(\n",
    "            {\n",
    "                'input': [vector_itos(tokenized_sentence) for tokenized_sentence in segments_X],\n",
    "                'output':list(nsp_y_hat),\n",
    "                'ground truth':list (nsp_y)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        mlm_data = pd.DataFrame(\n",
    "            {\n",
    "                'input': [vector_itos(tokenized_sentence) for tokenized_sentence in tokens_X],\n",
    "                'output':[\n",
    "                    ', '.join(\n",
    "                        list(\n",
    "                            vector_itos(tokenized_pred_words.argmax(dim=1))\n",
    "                        )\n",
    "                    ) for tokenized_pred_words in mlm_y_hat\n",
    "                ],\n",
    "\n",
    "                'ground truth': [\n",
    "                    ', '.join(\n",
    "                        list(vector_itos(tokenized_words))\n",
    "                    ) for tokenized_words in mlm_Y\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        break\n",
    "\n",
    "    return nsp_data, mlm_data\n",
    "\n",
    "nsp_data, mlm_data = demonstration(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_data[['output', 'ground truth']].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    'nsp_predictions': wandb.Table (nsp_data),\n",
    "    'mlm_predictions': wandb.Table (mlm_data)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_class_name(obj):\n",
    "    # taken from http://stackoverflow.com/\n",
    "    module = obj.__class__.__module__\n",
    "    if module is None or module == str.__class__.__module__:\n",
    "        return obj.__class__.__name__\n",
    "    return module + '.' + obj.__class__.__name__\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eea02655494ccfca9b943908b44a342bce2b5b6ec71a404df79d1eaeac6a6912"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
